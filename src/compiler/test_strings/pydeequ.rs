pub const PYTHON_PYDEEQU_RESULT_1: &str = "from pyspark.sql import SparkSession, DataFrame
from pydeequ import deequ_maven_coord, f2j_maven_coord
from pyspark.sql.functions import lit
from pydeequ.checks import Check, CheckLevel
from pydeequ.verification import VerificationSuite, VerificationResult


def column_level_checks_id(data_frame: DataFrame, spark_session: SparkSession) -> tuple[str, DataFrame | None]:
    try:
        check = Check(spark_session, CheckLevel.Warning,
                      \"Autogenerated check for column level rules for table Test and column Id\")
        check_result = VerificationSuite(spark_session).onData(data_frame).addCheck(
            check
            .satisfies(\"Id LIKE '%test%'\", \"check_like_pattern_Test_Id\", lambda x:x==1)
            .hasPattern(\"Id\", r\"test\", lambda x:x==1, \"check_contains_value_Test_Id\")
        ).run()

        result_df = VerificationResult.checkResultsAsDataFrame(spark_session, check_result)

        result_df = (result_df.withColumn(\"check_category\", lit(\"column level\"))
                     .withColumn(\"columns\", lit(\"test.id\")))
        return 'success', result_df

    except Exception as e:
        return f'failure: {e}', None


def column_level_checks_price(data_frame: DataFrame, spark_session: SparkSession) -> tuple[str, DataFrame | None]:
    try:
        check = Check(spark_session, CheckLevel.Warning,
                      \"Autogenerated check for column level rules for table Test and column Price\")
        check_result = VerificationSuite(spark_session).onData(data_frame).addCheck(
            check
            .satisfies(\"Price LIKE '%test%'\", \"check_like_pattern_Test_Price\", lambda x:x==1)
            .hasPattern(\"Price\", r\"[0-9]*test[0-9]*\", lambda x:x==1, \"check_has_pattern_Test_Price\")
        ).run()

        result_df = VerificationResult.checkResultsAsDataFrame(spark_session, check_result)

        result_df = (result_df.withColumn(\"check_category\", lit(\"column level\"))
                     .withColumn(\"columns\", lit(\"test.price\")))
        return 'success', result_df

    except Exception as e:
        return f'failure: {e}', None


def check_column_level(data_frame: DataFrame, spark_session: SparkSession) -> tuple[DataFrame | None, list[tuple[str, str]]]:
    column_level_checks_id_df = column_level_checks_id(data_frame, spark_session)
    column_level_checks_price_df = column_level_checks_price(data_frame, spark_session)
    checks = {
        'column_level_checks_id': column_level_checks_id_df,
        'column_level_checks_price': column_level_checks_price_df,
        }

    combined_result_df = None
    failed_checks = []

    for key, (is_success, data) in checks.items():
        if is_success == 'success':
            if combined_result_df is None:
                combined_result_df = data
            else:
                combined_result_df = combined_result_df.union(data)
        else:
            failed_checks.append((key, is_success))
    combined_result_df.show()
    return combined_result_df, failed_checks


def check_table(data_frame: DataFrame, spark_session: SparkSession) -> tuple[DataFrame | None, list[tuple[str, str]]]:
    return check_column_level(data_frame, spark_session)


if __name__ == '__main__':
    # example usage
    spark = (SparkSession.builder
             .config(\"spark.jars.packages\", deequ_maven_coord)
             .config(\"spark.jars.excludes\", f2j_maven_coord).appName('test').getOrCreate())
    df = spark.read.csv('./data/test.csv', header=True, inferSchema=True)
    check_table(df, spark)

    spark.sparkContext.stop()
    spark.stop()
";
